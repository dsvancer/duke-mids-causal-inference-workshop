{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e9808e",
   "metadata": {},
   "source": [
    "# Estimating Average Treatment Effects with ML\n",
    "\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/double_ml.png\" \n",
    "  alt=\"Confounding Relationships\" \n",
    "  style=\"width:300px;height:auto;\"\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\") \n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "\n",
    "## Double ML\n",
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load observational dataset\n",
    "observational_df = pd.read_pickle('../data/observational_df.pkl')\n",
    "# For reference\n",
    "potential_outcomes_df = pd.read_pickle('../data/potential_outcomes_df.pkl')\n",
    "\n",
    "# Identify columns\n",
    "customer_features = observational_df.drop(columns=['converted', 'upsell_marketing']).columns.to_list()\n",
    "target_outcome = 'converted'\n",
    "\n",
    "print('Customer Features: ', customer_features)\n",
    "\n",
    "observational_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba191a5",
   "metadata": {},
   "source": [
    "## Causal Assumptions\n",
    "All causal models share the following data assumptions\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/causal_assumptions.png\" \n",
    "  alt=\"Causal Assumptions\" \n",
    "  style=\"width:750px;height:auto;\"\n",
    "> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75650353",
   "metadata": {},
   "source": [
    "## Biased Estimates of Causal Effects\n",
    "\n",
    "Estimating a causal effect by simply calculating $E(Y|T=1) - E(Y|T=0)$ will give extremely biased results on most observational data. This is primarily due to confounding, where other variables are causing both treatment assignment and outcomes. This will mask the true average treatment effect of the treatment variable.\n",
    "\n",
    "It is always important to ask: is the $Y^{0}$ outcome the same, on average, within the population who received treatment and the one which did not? \n",
    "\n",
    "Another way to ask this would be: If I didn't give the treatment to the customers in my treatment group, would their results be similar to the results of the non-treated group?\n",
    "\n",
    "If the answer is no, then you have confounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check treatment effect\n",
    "signup_rate_by_treatment = observational_df.groupby('upsell_marketing')['converted'].mean()\n",
    "print(f\"Signup rates by treatment group: {signup_rate_by_treatment.apply('{:.1%}'.format)}\")\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "observational_df.groupby('upsell_marketing')['converted'].mean().plot(kind='bar', color=['tab:blue', 'tab:green'])\n",
    "plt.title(\"Signup Rate by Upsell Message Exposure\")\n",
    "plt.xlabel(\"Upsell Marketing (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Conversion Rate\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045384eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference in conversion rates by teatment type\n",
    "biased_lift = (\n",
    "    (observational_df['converted'][observational_df.upsell_marketing==1].mean()) - \n",
    "    (observational_df['converted'][observational_df.upsell_marketing==0].mean())\n",
    ")\n",
    "\n",
    "# True average treatment effect based on ITE\n",
    "actual_lift = potential_outcomes_df.individual_treatment_effect.mean()\n",
    "print(\n",
    "    f'Biased Marketing Lift: {biased_lift:.2%}',\n",
    "    f'Acutal Marketing Lift: {actual_lift:.2%}', \n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba67842",
   "metadata": {},
   "source": [
    "## Inverse Propensity Score Matching\n",
    "\n",
    "The IPW Estimator seeks to estimate $Y^{1}$ and $Y^{0}$ using a weighted average of the treated and untreated groups in our data. Let's explore how this is done in detail by calculating the estimate by hand using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc192274",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.set(\n",
    "    title='Inverse Propensity Score Weights by Treatment\\n',\n",
    "    xlabel='\\nEstimated Propensity Score (p) \\n$P(T=1 | X)$',\n",
    "    ylabel='Inverse Propensity Weight\\n',\n",
    "    xlim=(0,1),\n",
    "    ylim=(0, 40)\n",
    ")\n",
    "\n",
    "\n",
    "propensity_score = np.arange(0.01, 1.0, 0.001)\n",
    "t_1_weight = 1/propensity_score\n",
    "t_0_weight = 1/(1 - propensity_score)\n",
    "\n",
    "ax.plot(propensity_score, t_1_weight, '-', color='tab:blue', label='IPW for Treatment = 1 ($\\\\frac{1}{p} $)');\n",
    "ax.plot(propensity_score, t_0_weight, '-', color='tab:orange', label='IPW for Treatment = 0 ($\\\\frac{1}{1 - p} $)');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27639e86",
   "metadata": {},
   "source": [
    "Let's fit a simple logistic regression model to estimate the probability of receiving treatment (seeing an upsell marketing message) and go through the steps of IPW estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71846d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "propensity_logistic = LogisticRegression(random_state=42)\n",
    "\n",
    "# Fit the model to observational data\n",
    "propensity_logistic.fit(observational_df[customer_features], observational_df[target_outcome])\n",
    "\n",
    "# Use the predict_proba method to obtain estimated propensity scores\n",
    "propensity_scores = propensity_logistic.predict_proba(observational_df[customer_features]).clip(min=0.01, max=0.99)\n",
    "\n",
    "print(f'Columns (Outcomes of upsell_marketing): {propensity_logistic.classes_}')\n",
    "propensity_scores[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d460ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights for each customer\n",
    "weighted_df = (\n",
    "    observational_df\n",
    "    .assign(\n",
    "        prob_no_upsell=propensity_scores[:, 0],\n",
    "        prob_upsell=propensity_scores[:, 1])\n",
    "    .pipe(lambda df:\n",
    "          df.assign(\n",
    "              ipw=df['upsell_marketing']\n",
    "              .case_when([(df['upsell_marketing'] == 0, 1/df['prob_no_upsell']),\n",
    "                          (df['upsell_marketing'] == 1, 1/df['prob_upsell'])])\n",
    "          )\n",
    "    )\n",
    ")\n",
    "\n",
    "weighted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88a0b2",
   "metadata": {},
   "source": [
    "### Positivity Assumption\n",
    "Now we check the range of propensity scores make sure that the Upsell/Non-Upsell groups overlap on these scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.set(\n",
    "    title='Treatment Overlap Check',\n",
    "    ylabel='Proportion of Customers',\n",
    "    xlabel='Propensity Score\\nP(Upsell Marketing = 1)'\n",
    "    )\n",
    "\n",
    "\n",
    "sns.histplot(\n",
    "    weighted_df.query(\"upsell_marketing==1\")[\"prob_upsell\"],\n",
    "    stat='proportion', binrange=(0, 0.6), bins=30, alpha=0.5,\n",
    "    label=\"Upsell Marketing\", color='tab:blue', ax=ax)\n",
    "\n",
    "sns.histplot(\n",
    "    weighted_df.query(\"upsell_marketing==0\")[\"prob_upsell\"], \n",
    "    stat='proportion', binrange=(0, 0.6), bins=30, alpha=0.3,\n",
    "    label=\"Non-Upsell\", color='tab:orange', ax=ax)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each group based on treatment\n",
    "upsell_df = weighted_df.query('upsell_marketing == 1')\n",
    "non_upsell_df = weighted_df.query('upsell_marketing == 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac930e9",
   "metadata": {},
   "source": [
    "Now we calculate the weighted mean of the outcome variable, `converted`, using the inverse propensity weights. It is important to do this separately for each treatment group. The weighted mean must be normalized within each group to be a valid estimate of $Y^{1}$ and $Y^{0}$, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91659eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = np.average(upsell_df['converted'], weights=upsell_df['ipw'])\n",
    "y_0 = np.average(non_upsell_df['converted'], weights=non_upsell_df['ipw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a60bb3",
   "metadata": {},
   "source": [
    "Not bad! Our estimate is 2.18%, while the true ATE is 2.64%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Y(1) Estimate: {y_1:.2%}',\n",
    "    f'Y(0) Estimate: {y_0:.2%}',\n",
    "    f'ATE [Y(1) - Y(0)]: {y_1 - y_0:.2%}',\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03af327",
   "metadata": {},
   "source": [
    "## Double Machine Learning Model Families\n",
    "\n",
    "The `doubleml` package has a number of models that can be used for various causal effects estimation tasks based on the assumed casual mechanisms present in observational data. \n",
    "\n",
    "All available model types are listed in their [model documentation](https://docs.doubleml.org/stable/guide/models.html)\n",
    "\n",
    "We will be used the PLR model, which is the most common model when we have confounding due to customer features. The causal diagram for this model is shown below\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/plr_model.png\" \n",
    "  alt=\"Confounding Relationships\" \n",
    "  style=\"width:550px;height:auto;\"\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1fe75",
   "metadata": {},
   "source": [
    "### Creating DoubleML Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_data = (\n",
    "    DoubleMLData(\n",
    "        data=observational_df,\n",
    "        y_col='converted',\n",
    "        d_cols='upsell_marketing',\n",
    "        x_cols=customer_features,\n",
    "        use_other_treat_as_covariate=False)\n",
    ")\n",
    "\n",
    "print(dml_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8459482",
   "metadata": {},
   "source": [
    "### Defining the Various ML Models\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img \n",
    "  src=\"../assets/double_ml_process.png\" \n",
    "  alt=\"Double ML Process\" \n",
    "  style=\"width:750px;height:auto;\"\n",
    "> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model components\n",
    "## Set random seed for reproducability\n",
    "np.random.seed(42)\n",
    "\n",
    "# Outcome and treatment models\n",
    "outcome_model = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    max_features=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "treatment_model = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    max_features=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# DML model\n",
    "dml_model = DoubleMLPLR(\n",
    "    dml_data,\n",
    "    ml_l=outcome_model,\n",
    "    ml_m=treatment_model,\n",
    "    n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "dml_model.fit();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View treatment effect estimates\n",
    "dml_model.summary.style.format({\n",
    "    'coef': '{:.2%}',\n",
    "    'std err': '{:.2%}',\n",
    "    't': '{:,.2f}',\n",
    "    'P>|t|': '{:,.4f}',\n",
    "    '2.5 %': '{:.2%}',\n",
    "    '97.5 %': '{:.2%}'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
